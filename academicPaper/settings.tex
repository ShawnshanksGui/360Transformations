\newcommand\testbitrateBudget{6}
\newcommand\testbitrateBudgetPercentage{\SI{75}{\percent}}
\section{System Settings}
\label{sec:settings}

The preparation of 360-degree videos for viewport-adaptive streaming
relies on multiple parameters. We distinguish between global
parameters (the number of \acp{QEC}, the number of
representations, the segment length and the geometric layout)
and local (\emph{per representation}) parameters (the target bit-rate,
the number of different qualities in a representation, the quality
arrangement onto the different faces of a geometric layout). We \GS{will not
be}{} comprehensive regarding the selection of all these
parameters here. Some of them require a deeper study related to signal
processing, while others depend on business
considerations and infrastructure investment. In this paper, we
restrict our attention to three key questions: What is the best
geometric layout to support quality-differentiated 360-degree video?
What is the best segment length to support head movements, while
maintaining low management overhead? What is the best number of
\acp{QEC} $n$ to reduce the induced storage requirements, while
\GS{offering}{} a good \ac{QoE}? To answer these three questions, we have
developed a software tool and we have used a dataset \GS{from a real \ac{VR} system}.
%real users
%watching 360-degree videos.

%\subsection{Software and Dataset}

\parag{Dataset}We graciously received from Jaunt, Inc a dataset
recording the head movements of real users watching 360-degree videos.
The dataset is the same as the one used by~\citet{yu_framework_2015}. It comprises
eleven omnidirectional videos that are ten seconds long. These videos
are typical of \ac{VR} systems. The dataset contains
the head movements of eleven people who were asked to watch the videos
on a state-of-the-art \ac{HMD} (Occulus Rift DK2). The subjects were
standing and they were given the freedom to turn around, so the head
movements are of wider importance than if they were asked to watch the
video while sitting. Given the length of the video and the
experimental conditions, we believe that the head movements thus
correspond to a configuration of wide head movements, which
is the most challenging case for our viewport-adaptive
system. \citet{yu_framework_2015} studied the most frequent head
positions of users. We are interested here in head movements
during the length of a segment.
%They show that the FoV center is around the equator of the spherical
%video during the majority of time but that lateral movements are
%frequent.

\parag{Software}We have developed our own tool to manipulate the main
concepts of viewport-adaptive streaming. Since the code is publicly
available,\footnote{\url{https://github.com/xmar/360Transformations}}
the software can be used and enhanced to make further studies and to
develop real systems. The main features include: $(i)$
\emph{Projection from a spherical video onto any of the four geometric
layouts and vice versa}. The spherical video is the pivot format from
which it is possible to project to any layout. 
%Note that the majority
%of 360-degree videos that are currently available are encoded and
%stored after an equirectangular projection, but our tools enables
%re-projecting these videos to another layout. 
\XC{Our tool rotate the
video so that the \ac{QEC} center is always at the same position on
the $2$D layouts.}{} $(ii)$ \emph{Adjusting the video quality for each
geometric face of any layout}. \GS{For each face}, we set the resolution in
number of pixels and the target encoding bit-rate. 
%Each face is
%encoded into its own video track to allow for different bit-rate
%targets per face. 
$(iii)$ \emph{Viewport extraction for any
\ac{FoV} center on the sphere}. \XC{It includes the decoding, rescaling
and ``projection'' of each face of the input video to extract the
viewport. This tool support extraction of \ac{FoV} that overlap on
multiple faces with different resolution and bit-rate target.}{}

\subsection{Geometric Layout}

We report now the experiment of measuring the video quality of
viewports, extracted from 360-degree videos
projected onto various geometric layouts and with various face quality
arrangements. We used two reference videos.
\begin{itemize}[leftmargin=7pt, itemindent=0pt, topsep=2pt, itemsep=-3pt]

   \item \emph{The original video at full quality}%
   :\footnote{\url{https://youtu.be/yarcdW91djQ}} 
	We extract viewports at 1080p resolution from this 4k
   equirectangular video. These are the reference (original)
   video we used for the objective video quality assessment:
   \ac{MS-SSIM}~\cite{wang2003multiscale} and
   \ac{PSNR}. 
%   \ac{MS-SSIM} compares image by image the structural
%   similarity between a video and a reference video. It computes a
%   value between \num{0} and \num{1}, \num{1} being two identical videos.
%   The \ac{PSNR} measures the average error of pixel intensities
%   between a video and a reference video. It computes a value in
%   decibel (\si[mode=text]{\decibel}), $+\infty$ being two identical videos.}{}. 
   Since we
   compare several encoded versions of the \emph{same} video against the
   original, these well-known tools provide a fair performance evaluation of video distortion.

   \item \emph{The same video re-encoded at a target bit-rate}. 
   \XC{It is what a regular delivery system would deliver for the same bit-rate budget (here \SI{\testbitrateBudget}{\mega
   bps} being \testbitrateBudgetPercentage{} of the original video bit-rate).}{} 
   We re-encoded the original full-quality video with \ac{HEVC}
   by specifying this bit-rate target. We call it \emph{uniEqui} to
   state that, in this video, the quality is uniform.
%ith this bit-rate
%\emph{budget}, our competitor is the original equirectangular video re-encoded with
%\ac{HEVC} by specifying this bit-rate target. We call it \emph{uniEqui} to state that, in this
%video, the quality is uniform.
\end{itemize}

%Recall that our ultimate motivation
%is to reduce the bit-rate of the delivered video, while maintaining a high video quality.

\XC{We define a \textit{quality-differentiated layout} as}{} the
combination of a geometric layout and video quality arrangement onto
the geometric faces. The performance of the layout can be studied with
regards to two aspects: $(i)$ \emph{the best viewport quality}, which
is the quality of the extracted viewport when the \ac{FoV} center and
the \ac{QEC} perfectly matches, $(ii)$ and the \emph{sensitivity to
head movements}, which is the degradation of the viewport quality when
the distance between the FoV center and the \ac{QEC} increases. To
examine both aspects, we chose one origin point \ac{QEC} in the
spherical video. \XC{Next,}{} we iterated over the \emph{orthodromic
distance} $d$, which increases from \numrange{0}{\pi} (the furthest
point). For each value $d$, we randomly picked forty \ac{FoV} centers
at distance $d$ from the origin \ac{QEC} and we extracted the
viewport, which we compared to the same viewport extracted from the
full-quality original video, to get an objective video quality
measure.


%We represent in Figure~\ref{fig:dist_quality_psnr} the difference of \ac{PSNR} between
%the extracted viewport from our quality-differentiated layouts and the same viewport extracted from
%the reference \textit{uniEqui} layout. For each geometric layout (equirectangular
%panorama with $8\!\times\! 8$ tiles, cube map, pyramid, and dodecahedron), we have tested
%numerous quality arrangements with respect to the overall bit-rate budget. We selected
%here the ``best" arrangement for each layout.

We represent in Figure~\ref{fig:dist_quality} the video quality
(measured by \acs{MS-SSIM}) of the viewport that is extracted from our
quality-differentiated layouts (equirectangular panorama with
$8\!\times\! 8$ tiles, cube map, pyramid, and dodecahedron). We also
represent by a thin horizontal line the video quality of the same
viewport extracted from the \textit{uniEqui} layout (it does
not depend on the distance since the quality is uniform). For each
geometric layout, we have tested numerous quality arrangements with
respect to the overall bit-rate budget. We selected here the ``best"
arrangement for each layout. \XC{For the cube map, 
the
\ac{QEC} is located at the center of a face. This face is set at full
quality (same bit-rate target as the same portion of the original
video), and the other faces at \SI{25}{\percent} of the
full quality target}. 
%For equirectangular with tiles, the \ac{QEC} is
%located at the center of a tile. $3\!\times\! 3$ tiles arround the
%\ac{QEC} are set at full quality and the other at low quality. For the
%dodecahedron, the \ac{QEC} is located on a node. The $3$ faces that
%contain this node are set at full quality and the other at low
%quality.}{}

\begin{figure}
    \input{plots/distance_quality.tex}
%    \caption{Average \acs{PSNR} gap compared to the \emph{uniEqui} layout, depending on the distance from the \acs{QEC}. Global bit-rate budget \SI{\testbitrateBudget}{\mega bps}}
       \caption{Average \acs{MS-SSIM} depending on the distance to the \acs{QEC} for the four geometric layouts. Global bit-rate budget \SI{\testbitrateBudget}{\mega bps}}
    \label{fig:dist_quality}
\end{figure}

The projection on a cube map appears to be the best choice for the
\ac{VR} provider. The quality of the viewport when the \ac{QEC} and
the \ac{FoV} center matches ($d=0$) is above \num{0.98}, which
corresponds to imperceptible distortion relative to the full quality
video. For all layouts, the quality decreases when the distance $d$
increases but the quality for the cube map layout is always the
highest. Note that the pyramid projection (the layout chosen
by Facebook~\cite{facebook}) is especially sensitive to head movements.
%The video quality for all layouts save for the cube map is lower than
%that for the standard \emph{uniEqui} for distance greater than
%\num{1}.
\GS{The viewport extracted from a cube map projection has a better quality
than that extracted from the \emph{uniEqui} for \ac{FoV} center at up to 
\num{2} units from the \ac{QEC} while the other layouts are not
worthy for \ac{FoV} centers at only \num{1} unit of the \ac{QEC}.
%\XC{Here the cube map projection has better results
%probably because this layout have bigger faces. The bigger the face
%is, the more redundancies in the face are available and so the better
%the video codec exploits the same bit-rate budget.}{} 
We study next the interplay between this distance, the segment length and the number of \acp{QEC}}.

\subsection{Segment Length}
\label{subsec:segmentLength}

The segment length is a key aspect of viewport-adaptive streaming.
Long segments are easier to manage and better for video encoding, but
short segments enable fast re-synchronisation to head movement. With
respect to Figure~\ref{fig:dist_quality}, the segment length
should be chosen so that the distance between the \ac{FoV} center and
the \ac{QEC} are rarely higher than \num{1.5}~distance units.

Given the dataset, we show the distribution of head movements for
various segment lengths in Figure~\ref{cdf-dataset}. For each video
and person watching it, we set timestamps that correspond to the
starting time of a video segment, \textit{i.e.,} the time at which the
users select a \ac{QEC}. Then, we measure the orthodromic distance
between this initial head position and every \ac{FoV} center during
the next $x$ seconds, where $x$ is the segment length. In
Figure~\ref{cdf-dataset}, we show the \ac{CDF} of the time spent at a
distance $d$ from the initial head position. 
A point at $(1.5,0.6)$
means that, on average, users spend \SI{60}{\percent} of their time
with a \ac{FoV} center at less than \num{1.5}~distance units from the
\ac{FoV} center on the beginning of the segment.

\begin{figure}
\centering
\input{plots/cdf-dataset.tex}
\caption{CDF of the time spent at distance $d$ from the head position on the beginning of the
segment, for various segment lengths.}\label{cdf-dataset}
\end{figure}

Our main observation is that viewport-adaptive streaming requires
short segment lengths, typically smaller than \SI{3}{\second}. Indeed,
for a segment length of five seconds, users spend on average half of
their time watching at a position that is at more than
\num{1.3}~distance units away from the initial head position, which
results in a degraded video quality. A segment length of
\SI{2}{\second} appears to be a good trade-off: \SI{92}{\percent} of
users never diverged to a head position that is further than
\num{2}~distance unit away from the initial head position, and users
can experience the full video quality three quarters of the time (head distance
lesser than \num{0.7}~distance unit). Please recall that our dataset
captures a challenging experiment for our system. We can expect
narrower head movements, and thus longer possible segment lengths, for
sitting users and longer videos.

%such
%a length is acceptable regarding the management at the server (the
%number of segments to deal with and the size of the \ac{MPD}) and at
%the client (the frequency of representation selection and the number
%of requests to send). Furthermore, for a \SI{2}{\second}-long segment,

\subsection{Number of \acp{QEC}}

The number of \acp{QEC} $n$ represents another key trade-off. The more
\acp{QEC} there are, the better the coverage of the spherical video
is, and thus the better the viewport quality will be due to a better
match between the \ac{QEC} and the \ac{FoV} center. However,
increasing the number of \acp{QEC} also means increased storage and
management requirements at the server (and a longer \ac{MPD} file).

% \XC{The choice of the position of $n$ \ac{QEC} on a sphere is not
% trivial. We want them to uniformly distributed over the spherical
% video. This definition is hazy: the results highly depends on what we
% consider to be the neighborhood of each \ac{QEC} and on our notion of distance. This problem
% is a classic in particle physic: minimizing the energy of $n$ charged
% particles on a sphere~\cite{rakhmanov1994electrons} also known as the
% Thomson problem. We chose to use a
% solver~\footnote{http://particules.sectionpc.info/bin/Particules.zip}
% for the Thomson problem to determine the position of each \ac{QEC}.}{}

We represent in Figure~\ref{fig:QEC} the median \ac{PSNR} difference
between the viewport extracted from the cube map layout and the
same viewport extracted from the \emph{uniEqui} layout with the same
overall bit-rate budget. To modify the number of \acp{QEC}, we set a
number $n$, then we determined the position of the $n$ \acp{QEC} 
using the Thomson positioning problem~\cite{rakhmanov1994electrons}.
%\footnote{http://particules.sectionpc.info/bin/Particules.zip}
%, and we generated $n$ quality-differentiated video representations from these $n$ \acp{QEC}.
For each head position in the dataset, we computed the distance
between the \ac{FoV} center and the \ac{QEC} that was chosen at the
beginning of the segment and we computed the viewport quality
accordingly.
%For each video and for
%
%
%We show in Figure~\ref{fig:QEC} the average \acs{MS-SSIM} quality of the viewports as
%would the users of our datasets see them if the 360-degree video had been prepared
%with a given number of QECs $n$.
%%We used the best configuration, \textit{i.e.}, a cube map projection and \SI{2}{\second} segment.
%\XC{We used the best projection, \textit{i.e.} the cube map projection.}{}


\begin{figure}
\centering
\input{plots/qecNbToQoE.tex}
\caption{Median \acs{PSNR} gap between the viewports of the cube map layout and the \textit{uniEqui} depending
on the number of QECs.
Bit-rate: \SI{\testbitrateBudget}{\mega bps}}
\label{fig:QEC}
\end{figure}

The best number of \acp{QEC} in this configuration is between
\numlist{5;7}. The gains that are obtained for higher number of
\acp{QEC} are not significant enough to justify the induced storage
requirements (in particular not $30$~\acp{QEC} as in the Facebook
system~\cite{facebook}). Having multiple  \acp{QEC} provides higher
quality gains for short segments, due to the better re-synchronization
between the \acp{QEC} and the \ac{FoV} centers. Note also that a
significant part of these gains stems from the cube map layout
projection.
%
%is between $6$ and $7$.
%\XC{More \acp{QEC} improve slightly the quality for a linear growth in the
%storage needs. Moreover the longer the segment duration is, the lesser
%the number of \acp{QEC} influences the quality. For a \SI{2}{\second}
%segment the number of \ac{QEC} is still significant.}
