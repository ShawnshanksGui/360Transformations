\newcommand\testbitrateBudget{6}
\newcommand\testbitrateBudgetPercentage{\SI{75}{\percent}}
\section{System Settings}
\label{sec:settings}

The preparation of $360$-degree videos for viewport-adaptive streaming
relies on multiple parameters. We distinguish between global
parameters (the number of \acp{QEC}, the number of
representations, the segment length and the geometric layout)
and local (\emph{per representation}) parameters (the target bit-rate,
the number of different qualities in a representation, the quality
arrangement of different faces of a geometric layout). We \GS{will not
be}{} comprehensive regarding the selection of all these
parameters here. Some of them require a deeper study related to signal
processing, while others depend on business
considerations and infrastructure investment. In this paper, we
restrict our attention to three key questions: What is the best
geometric layout to support quality-differentiated $360$-degree video?
What is the best segment length to support head movements, while
maintaining low management overhead? What is the best number of
\acp{QEC} $n$ to reduce the induced storage requirements, while
offering a good \ac{QoE}? To answer these three questions, we have
developed a software tool and used a dataset from a real \ac{VR} system.


\parag{Dataset}We graciously received from Jaunt, Inc a dataset
recording the head movements of real users watching $360$-degree videos.
The dataset is the same as the one used by~\citet{yu_framework_2015}. It comprises
eleven omnidirectional videos that are ten seconds long. These videos
are typical of \ac{VR} systems. The dataset contains
the head movements of eleven people who were asked to watch the videos
on a state-of-the-art \ac{HMD} (Occulus Rift DK2). The subjects were
standing and they were given the freedom to turn around, so the head
movements are of wider importance than if they were asked to watch the
video while sitting. Given the length of the video and the
experimental conditions, we believe that the head movements thus
correspond to a configuration of wide head movements, which
is the most challenging case for our viewport-adaptive
system. \citet{yu_framework_2015} studied the most frequent head
positions of users. We are interested here in head movements
during the length of a segment.

\parag{Software}We have developed our own tool to manipulate the main
concepts of viewport-adaptive streaming. Since the code is publicly
available,\footnote{\url{https://github.com/xmar/360Transformations}}
the software can be used to make further studies and to
develop real systems. The main features include:
\begin{itemize}
  \item \emph{Projection from a spherical video onto any of the four geometric
layouts and vice versa}. The spherical video is the pivot format from
which it is possible to project to any layout.
Our tool rotates the
video so that the \ac{QEC} is always at the same position on
the $2$D layout.

  \item \emph{Adjusting the video quality for each
geometric face of any layout}. For each face, we set the resolution in
number of pixels and the target encoding bit-rate.

  \item \emph{Viewport extraction for any
\FoV{} center on the sphere}. It includes the decoding, rescaling
and ``projection'' of each face of the input video to extract the
viewport. This tool support extraction of \FoV{} that overlap on
multiple faces with different resolution and bit-rate target.
\end{itemize}

\subsection{Geometric Layout}

We report now the experiment of measuring the video quality of
viewports, extracted from $360$-degree videos
projected onto various geometric layouts and with various face quality
arrangements. We used two reference videos.
\begin{itemize}
   \item \emph{The original equirectangular video at full quality}:\footnote{\url{https://youtu.be/yarcdW91djQ}}
	We extract viewports at $1080$p resolution from this $4$K
   equirectangular video, which represents the reference (original)
   video we used for the objective video quality assessment.

   \item \emph{The same equirectangular video re-encoded at a target bit-rate}.
   It is what a regular delivery system would deliver for the same bit-rate budget (here \SI{\testbitrateBudget}{\mega
   bps} being \testbitrateBudgetPercentage{} of the original video bit-rate).
   We re-encoded the original full-quality video with \ac{HEVC}
   by specifying this bit-rate target. We call it \emph{uniEqui} to
   state that, in this video, the quality is uniform.
\end{itemize}

The performance of the layout can be studied with
regards to two aspects: $(i)$ \emph{the best viewport quality}, which
is the quality of the extracted viewport when the \FoV{} center and
the \ac{QEC} perfectly matches, $(ii)$ and the \emph{sensitivity to
head movements}, which is the degradation of the viewport quality when
the distance between the \FoV{} center and the \ac{QEC} increases.
To examine both aspects, we select one \ac{QEC} on the spherical video.
We chose one orthodromic distance $d$ that will vary from \numrange{0}{\pi}.
We extract a ten seconds long viewport video, at distance $d$
from the \ac{QEC}, at the same spherical position on the original
equirectangular video and on the tested video. We used two objective
video quality metrics to measure the quality of the extracted viewport
compared to the original full quality viewport:
\ac{MS-SSIM}~\cite{wang2003multiscale} and
\ac{PSNR}.
\ac{MS-SSIM} compares image by image the structural
similarity between this video and the reference video. The \ac{PSNR} measures the average error of pixel intensities
between this video and the reference video. Since we
compare several encoded versions of the \emph{same} viewport against the
original, these well-known tools provide a fair performance evaluation of viewport distortion.
We perform multiple quality assessment (typically forty) at the same distance $d$ but at different positions and average the result.


% The performance of the layout can be studied with
% regards to two aspects: $(i)$ \emph{the best viewport quality}, which
% is the quality of the extracted viewport when the \FoV{} center and
% the \ac{QEC} perfectly matches, $(ii)$ and the \emph{sensitivity to
% head movements}, which is the degradation of the viewport quality when
% the distance between the \FoV{} center and the \ac{QEC} increases. To
% examine both aspects, we chose one origin point \ac{QEC} in the
% spherical video. Next, we iterated over the \emph{orthodromic
% distance} $d$, which increases from \numrange{0}{\pi} (the furthest
% point). For each value $d$, we randomly picked forty \FoV{} centers
% at distance $d$ from the origin \ac{QEC} and we extracted the
% viewports, which we compared to the same viewports extracted from the
% full-quality original video, to get an objective video quality
% measure.



We represent in Figure~\ref{fig:dist_quality} the video quality
(measured by \acs{MS-SSIM}) of the viewport that is extracted from our
quality-differentiated layouts (equirectangular panorama with
$8\!\times\! 8$ tiles, cube map, pyramid, and dodecahedron). We also
represent by a thin horizontal line the video quality of the same
viewports extracted from the \textit{uniEqui} layout (it does
not depend on the distance since the quality is uniform). For each
geometric layout, we have tested numerous quality arrangements with
respect to the overall bit-rate budget. We selected here the ``best"
arrangement for each layout. For the cube map,
the
\ac{QEC} is located at the center of a face. This face is set at full
quality (same bit-rate target as the same portion of the original
video), and the other faces at \SI{25}{\percent} of the
full quality target.

\begin{figure}
    \input{plots/distance_quality.tex}
       \caption{Average \acs{MS-SSIM} depending on the distance to the \acs{QEC} for the four geometric layouts. Global bit-rate budget \SI{\testbitrateBudget}{\mega bps}}
    \label{fig:dist_quality}
\end{figure}

The projection on a cube map appears to be the best choice for the
\ac{VR} provider. The quality of the viewport when the \ac{QEC} and
the \FoV{} center matches ($d=0$) is above \num{0.98}, which
corresponds to imperceptible distortion relative to the full quality
video. For all layouts, the quality decreases when the distance $d$
increases but the quality for the cube map layout is always the
highest. Note that the pyramid projection (the layout chosen
by Facebook~\cite{facebook}) is especially sensitive to head movements.
The viewport extracted from a cube map projection has a better quality
than that extracted from the \emph{uniEqui} for \FoV{} center for up to
\num{2} units from the \ac{QEC} while the other layouts viewports increase a video quality for only \num{1} unit of the \ac{QEC}.
We study next the interplay between this distance, the segment length and the number of \acp{QEC}.

\subsection{Segment Length}
\label{subsec:segmentLength}

The segment length is a key aspect of viewport-adaptive streaming.
Long segments are easier to manage and better for video encoding, but
short segments enable fast re-synchronisation to head movement. With
respect to Figure~\ref{fig:dist_quality}, the segment length
should be chosen such that the distance between the \FoV{} center and
the \ac{QEC} are rarely higher than \num{1.5}~distance units.

Given the dataset, we show the distribution of head movements for
various segment lengths in Figure~\ref{cdf-dataset}. For each video
and person watching it, we set timestamps that correspond to the
starting time of a video segment, \textit{i.e.,} the time at which the
users select a \ac{QEC}. Then, we measure the orthodromic distance
between this initial head position and every \FoV{} center during
the next $x$ seconds, where $x$ is the segment length. In
Figure~\ref{cdf-dataset}, we show the \ac{CDF} of the time spent at a
distance $d$ from the initial head position.
A point at $(1.5,0.6)$
means that, on average, users spend \SI{60}{\percent} of their time
with a \FoV{} center at less than \num{1.5}~distance units from the
\FoV{} center on the beginning of the segment.

\begin{figure}
\centering
\input{plots/cdf-dataset.tex}
\caption{CDF of the time spent at distance $d$ from the head position on the beginning of the
segment, for various segment lengths.}\label{cdf-dataset}
\end{figure}

Our main observation is that viewport-adaptive streaming requires
short segment lengths, typically smaller than \SI{3}{\second}. Indeed,
for a segment length of \SI{5}{\second}, users spend on average half of
their time watching at a position that is more than
\num{1.3}~distance units away from the initial head position, which
results in a degraded video quality. A segment length of
\SI{2}{\second} appears to be a good trade-off: \SI{92}{\percent} of
users never diverged to a head position that is further than
\num{2}~distance unit away from the initial head position, and users
can experience the full video quality three quarters of the time (head distance
lesser than \num{0.7}~distance unit). Please recall that our dataset
captures a challenging experiment for our system. We can expect
narrower head movements, and thus longer possible segment lengths, for
sitting users and longer videos. Note also that these results are consistent
with the head movement prediction from~\citet{allthings}, who showed that 
prediction accuracy drops for time periods greater than \SI{2}{\second}.

\subsection{Number of \acp{QEC}}

The number of \acp{QEC} $n$ represents another key trade-off. The more
\acp{QEC} there are, the better the coverage of the spherical video
is, and thus the better the viewport quality will be due to a better
match between the \ac{QEC} and the \FoV{} center. However,
increasing the number of \acp{QEC} also means increased storage and
management requirements at the server (and a longer \ac{MPD} file).


We represent in Figure~\ref{fig:QEC} the median \ac{PSNR} difference
between the viewport extracted from the cube map layout and the
same viewport extracted from the \emph{uniEqui} layout with the same
overall bit-rate budget. To modify the number of \acp{QEC}, we set a
number $n$, then we determined the position of the $n$ \acp{QEC}
using the Thomson positioning problem~\cite{rakhmanov1994electrons}.
For each head position in the dataset, we computed the distance
between the \FoV{} center and the \ac{QEC} that was chosen at the
beginning of the segment and we computed the viewport quality
accordingly.
%%We used the best configuration, \textit{i.e.}, a cube map projection and \SI{2}{\second} segment.


\begin{figure}
\centering
\input{plots/qecNbToQoE.tex}
\caption{Median \acs{PSNR} gap between the viewports of the cube map layout and the \textit{uniEqui} depending
on the number of \acp{QEC}.
Bit-rate: \SI{\testbitrateBudget}{\mega bps}}
\label{fig:QEC}
\end{figure}

The best number of \acp{QEC} in this configuration is between
\numlist{5;7}. The gains that are obtained for higher number of
\acp{QEC} are not significant enough to justify the induced storage
requirements (in particular not $30$~\acp{QEC} as in the Facebook
system~\cite{facebook}). Having multiple  \acp{QEC} provides higher
quality gains for short segments, due to the better re-synchronization
between the \acp{QEC} and the \FoV{} centers. Note also that a
significant part of these gains stems from the cube map layout
projection.
