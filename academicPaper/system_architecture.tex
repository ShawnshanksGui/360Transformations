\input{newDelivery.tex}

\section{System architecture}

This section describes a system architecture of the proposed navigable 360-degree video delivery. The principles of the navigable video delivery are similar as in adaptive bit-rate
video systems such as \ac{DASH}\cite{Stockhammer11}. The server offers multiple versions of the same video
and the client selects the most appropriate version according to some criteria. %These versions
%are cut into second-long segments such that the client can regularly switch from one
%version to another.

The server takes as an input a 360-degree video in equirectangular format and transforms each frame into desired geometrical layout. From there it creates \textit{N} different video versions, each with a different QEC and encoded in \textit{k} different bitrates, as illustrated in Figure \ref{fig:newdelivery}. The server splits all such encoded video versions into second-long segments that are classified in \textit{N*k} representations (based on their respective resolution, bitrate, and QEC value), enabling the client to regularly switch from one representation to another.

The specifics of 360-degree video delivery are that only a portion of the delivered 360-degree video is actually displayed on the user's HMD (i.e., where the user looks at). Therefore, our proposal is that only this part of the video (around the QEC) is given in full quality, while the video quality gradually decreases when moving away from the QEC.

Similarly to DASH, the client periodically runs an adaptation algorithm to select the appropriate video representation. However, it needs to first select, based on the user's head movements and the information from the MPD file, the QEC of the video. This is an important addition to the bitrate adaptation logic, since the QEC determines the quality of the video that will be delivered and displayed to the user. Second, the client chooses the video representation that contains this QEC, whose bitrate fits the expected throughput for the next X seconds (i.e., X being the segment duration). Third, it sends a request to the server for the new segment in the chosen representation. The server replies with the requested 360-degree video version, from which the client extracts the FoV video, displaying it on the user's HMD. Note that this is possible with the state-of-the-art HMDs \cite{}.

Figure \ref{} depicts the proposed adaptation algorithm. The head movements are forward and backwards, side to side, and shoulder to shoulder, referred to as \emph{pitch}, \emph{yaw}, and \emph{roll}, respectively. The center of the \ac{FoV} is a point on the sphere, while the size of the FoV depends on the device (typically around 100$^\circ$ in state-of-the-art devices).

The adaptive video streaming systems are based on the "bet" that the selected representation will match the configuration for the next X seconds. The requested bitrate needs to match the current throughput for the duration of the segment, otherwise the user might experience the video stalls, interruptions, and delays. In the proposed navigable 360-degree video delivery the bet is not only that the throughput will accommodate the requested bitrate, but also that the chosen QEC will be close to the FOV center for the next X seconds. If the user's head moves during this time, the consequences for the user will be even worse than with the adaptive streaming, including the deteriorated video quality. A new FoV video will be extrapolated from the existing video version stored on the client. The longer the time between the requests, the longer the potential distance from the old to the new FoV center (i.e., FoV center distance) and the larger the video quality degradation of the displayed FoV video. Exactly how many seconds should pass until receiving a new video version depends on the viewer's head movements, video content (i.e., whether he/she watches a football match, news, or plays a computer game), and the experienced quality degradations variations, which needs to be investigated in the future work.

Many bitrate adaptation logic algorithms for adaptive streaming have been developed over time to optimize the user experience \cite{}. Most of these algorithms focus on predicting the expected throughput for the next few chunks in order to make optimal bitrate decisions for maximizing the Quality of Experience (QoE). Similarly to different bitrate adaptation algorithms, we expect that 360-degree video adaptation algorithms will be developed in the future to predict the user's head movements for the next few seconds in order to adjust the video segment size and arrange the qualities in 360-degree video accordingly. %This head movement prediction will be based on the user's previous head movements and the video content type. If the content is static, the user will not move his/her head so much compared to when watching an action move.


A DASH manifest file is extended with new information to support the navigable 360-degree video delivery. Each representation contains the QEC coordinates in degrees, besides the parameters that are already defined in the standard \cite{}. Similarly, an adaptation set is extended with geometrical layout and quality arrangement attributes, for video encoder to know how to encode the video. The proposed changes are added as new attributes of \textit{Representation} and \textit{AdaptationSet} tags, as illustrated in Figure \ref{fig:mpdChanges}. With such extensions we maintain the conformance with the standard, ensuring the interworking with the legacy DASH clients that can simply ignore these newly introduced attributes.


\begin{lstlisting}[language=xml, frame=single, backgroundcolor=\color{white}, caption=Extensions of MPD file]
<?xml version="1.0"?>
   <MPD>
    ...
        <AdaptationSet geometricLayout="cubeMap" qualityArrangement="4/1/1">
            <Representation id="1" qec="90,60" bandwidth="9876" width="1920" height="1080" frameRate="30">
                <SegmentList timescale="1000" duration="2000">
                ...
            </Representation>
        </AdaptationSet>
    </MPD>
</xml>
\end{lstlisting}
\label{fig:mpdChanges}


A video segment size determines how often requests are sent to the server. It typically ranges from 2 seconds to 10 seconds per segment. Short segments are good to quickly adapt to head movement and bandwidth changes, but the video encoding requires a higher number of segments and streaming of this video results in larger manifest files. Shorter segments also increase the network overhead by frequent requests, as well as network delay because of the round trip time that is needed to establish a TCP connection\footnote{In case of non-persistent HTTP connection TCP connection is established with the server after each request.} and request a segment (which can both take significant time in case of very short segment sizes). Longer segments are better in coding efficiency and quality than the shorter ones, however loose on the flexibility to adapt the stream to changes.

A user's head movements affect the "optimal" segment size selection and quality of the displayed video. If the user watches relatively static video content, his/her head will not move that much, leading to slower head movements. Slower head movements usually cause shorter FoV center distances, requiring only a small portion of video to be encoded in high quality (while the rest can be given in the lowest quality) in order to optimize the user experience. They also need less frequent video segment updates. On the contrary, faster head movements typically produce longer FoV center distances,  requiring larger portions of video to be encoded in higher quality and more frequent segment updates to optimize the user experience.
%If the user moves his head a lot, he needs to get more frequently segments with new QECs.

We study, in the remainder of the paper, the impact of the segment size/request rate and the FoV center distance on the viewport quality that is displayed to the user, using a single head motion dataset. More studies with different datasets and video content types are needed in the future to generalize these results. We are also interested to find out how many representations are needed to achieve a desired video quality, using the particular geometrical layout.
