\section{System architecture}

This section describes the system architecture of the proposed
navigable 360-degree video delivery framework.
% The principles of the navigable video delivery are similar as in
% adaptive bit-rate video systems such as \ac{DASH}\cite{Stockhammer11}.
% The server offers multiple versions of the same video
% and the client selects the most appropriate version according to some criteria. %These versions
% are cut into second-long segments such that the client can regularly switch from one
% version to another.

\parag{Server} The server takes as an input a 360-degree video in
equirectangular format and transforms each frame into a desired
geometrical layout. From there, it creates $n$ different video
versions, each with a different QEC and encoded in $k$ different
bitrates, as illustrated in Figure~\ref{fig:newdelivery}. The server
splits all such encoded video versions into segments, which are
classified in $n*k$ representations (based on their respective bitrate
and QEC value), enabling the client to regularly switch from one
representation to another. The quality of each segment around the QEC
is the highest, while the remaining part is encoded at lower quality.

\begin{figure}
   \centering
   \input{plots/newDelivery.tex}
   \caption{Viewort-adaptive streaming system: the server offers $n\times k$ representations ($n$ \acp{QEC} at $k$ bit-rates). The streaming session lasts for three segments from $s_1$ to $s_3$. The client moves her head from left to right, while her available bandwidth varies. For each segment, the client requests a representation that matches both the \ac{FoV} and the network throughput.}
   \label{fig:newdelivery}
\end{figure}

\parag{Client} The client moves his/her head over time, while the
available bandwidth changes. The possible head movements are forward
and backwards, side to side, and shoulder to shoulder, referred to as
\emph{pitch}, \emph{yaw}, and \emph{roll}, respectively. Changes in
the head orientation modify the FoV center, requiring a new viewport
to be displayed to the user. The viewport extraction is possible with
the state-of-the-art HMDs~\cite{fovhmds}. The client periodically
sends a request to the server for a new video segment in the
representation that matches the new FoV \textbf{and} the available
throughput.

\parag{Adaptation algorithm} Similarly to DASH, the client runs an
adaptation algorithm to select the video representation. It first
selects the QEC of the video based on the user's head movements and
the available QECs coordinates. This is an important addition to the
DASH bitrate adaptation logic, since the QEC determines the quality of
the video that will be delivered and displayed to the user. After
selecting the QEC, the client chooses the video representation that
contains this QEC and whose bitrate fits the expected throughput for
the next $x$ seconds (\textit{i.e.}, $x$ being the segment length). It
sends a request to the server for the new segment in the chosen
representation. The server replies with the requested 360-degree video
representation, from which the client extracts the viewport,
displaying it on the HMD, as illustrated in Figure
\ref{fig:newdelivery}. %The center of the \ac{FoV} is a point on the
sphere, while the size of the FoV depends on the device (typically
around 100$^\circ$ in state-of-the-art devices).

%\parag{Adaptive video streaming}
The adaptive video streaming system is based on the assumption that
the selected representation will match the client/network
configuration for the next $x$ seconds. The requested bitrate needs to
match the current throughput for the duration of the segment,
otherwise the user might experience video stalls, interruptions, and
delays. Many bitrate adaptation algorithms have been recently
developed~\cite{tian,probe_li_2014,miller,zou,liu}. In our approach,
we additionally assume that the chosen QEC will be close to the FOV
center for the next $x$ seconds. In this paper, we implement a simple
algorithm for QEC selection: we select the QEC that has the smallest
orthodromic distance\footnote{The shortest distance between two points
on the surface of a sphere, measured along the surface of the sphere. It is measured in the same unit as the radius of the sphere; we refer to "distance units" in the text.}
to the FoV center at the time the client runs the adaptation
algorithm.
%Most of these algorithms focus on predicting the expected throughput for the next few seconds in order to make optimal bitrate decisions.
Similarly as for bit-rate adaptation, we expect new 360-degree video
adaptation algorithms to be developed in the future to better predict
the user's head movement and adjust the video segment length and
qualities in 360-degree video accordingly.
%This head movement prediction will be based on the user's previous head movements and the video content type. If the content is static, the user will not move his/her head so much compared to when watching an action move.


\parag{Video segment length} A video segment length determines how
often requests can be sent to the server. It typically ranges from 1
second to 10 seconds per segment. Short segments enables quick
adaptation to head movement and bandwidth changes, but it increases
the overall number of segments and results in larger manifest files.
Shorter segments also increase the network overhead by allowing for
frequent requests, as well as the network delay because of the round
trip time that is needed to establish a TCP connection.
%\footnote{In case of non-persistent HTTP connection TCP connection is established with the server after each request.} and request a segment (which can both take significant time in case of short segment sizes).
Longer segments improve the coding efficiency and quality relative to
shorter ones, however they reduce the flexibility to adapt the video
stream to changes. We discuss segment length and head movement in
Section~\ref{subsec:segmentLength} based on a dataset.

% However, if the user's head moves during this time, the consequences for the user will be even worse than with the adaptive streaming, including the degraded video quality. A new FoV video will be extrapolated from the existing video version stored on the client. The longer the time between the requests, the longer the potential distance from the old to the new FoV center (i.e., FoV center distance), and the larger the quality degradation of the extracted FoV video. In order to improve quality of the displayed video in future, we plan to predict which QEC will be close to FoV center for the entire segment duration, showing this area in full quality.

%The specifics of 360-degree video delivery are that only a portion of the delivered 360-degree video is displayed on the user's HMD (i.e., where the user looks at). Therefore, only this part of the video (around the QEC) should be given in full quality, while the quality of the remaining video can be adjusted to the user's head movements and the viewing probability. A video that is mapped to a given geometric layout is split into viewing faces, each of which corresponds to specific viewing direction and can be assigned a different quality.

\parag{Extending the \ac{MPD} file} To implement the proposed
FoV-adaptive video streaming, we extended a DASH \ac{MPD} file with
new information, as illustrated in Listing~\ref{mpdChanges}. Each
representation contains the \texttt{QEC coordinates} in degrees,
besides the parameters that are already defined in the
standard~\cite{iso_iec}. Similarly, an adaptation set is extended with
\texttt{geometrical layout} and \texttt{quality arrangement}
attributes, for video encoder to know how to encode the video. The
geometrical layout is one of the projection types (equirectangular,
cubeMap, pyramid or rhombic dodecahadron), while the quality
arrangement takes a string value specifying the qualities of the
different faces. In this example, we propose to represent the number
of layout faces encoded in the high/medium/low qualities. The proposed
changes are added as new attributes of the \texttt{Representation} and
\texttt{AdaptationSet} tags. With such extensions, we maintain
conformance with the standard, ensuring interworking with legacy DASH
clients that can simply ignore these newly introduced attributes.

\begin {lstlisting} [language=xml, frame=single, backgroundcolor=\color{white}, caption=Extensions of MPD file, label=mpdChanges]
<?xml version="1.0"?>
   <MPD>
    ...
        <AdaptationSet geometricLayout="cubeMap" qualityArrangement="4/1/1">
            <Representation id="1" qec="90,60" bandwidth="9876" width="1920" height="1080" frameRate="30">
                <SegmentList timescale="1000" duration="2000">
                ...
            </Representation>
        </AdaptationSet>
    </MPD>
</xml>
\end{lstlisting}



%The choice of segment length depends on the head movements, which also depends on the type of the video. If the user watches relatively static video content, his/her head is less likely to move, leading to slower head movements. Slower head movements usually cause shorter FoV center distances, requiring only a small portion of video to be encoded in high quality (while the rest can be given in the lowest quality) in order to optimize the user experience. They also need less frequent video segment updates. On the contrary, faster head movements typically produce longer FoV center distances,  requiring larger portions of video to be encoded in higher quality and more frequent segment updates to optimize the user experience.

%\parag{Future adaptation algorithm development}


%If the user moves his head a lot, he needs to get more frequently segments with new QECs.

%In the remainder of the paper, we study the impact of the segment length and the FoV center distance on the FoV quality that is displayed to the user, using a single head motion dataset and a car video. We are aware that the optimal segment size greatly depends on the specific user's head movements, video content type (i.e., whether the user watches a football match, news, or plays a computer game), and the experienced quality degradations. Therefore, more studies with different datasets and video content types are needed in the future to generalize these results. We are also interested to find out how many representations are needed to achieve a desired video quality, using the particular geometrical layout.
