\section{System architecture}

This section describes a system architecture of the proposed navigable 360-degree video delivery. %The principles of the navigable video delivery are similar as in adaptive bit-rate
%video systems such as \ac{DASH}\cite{Stockhammer11}. The server offers multiple versions of the same video
%and the client selects the most appropriate version according to some criteria. %These versions
%are cut into second-long segments such that the client can regularly switch from one
%version to another.
The server takes as an input a 360-degree video in equirectangular format and transforms each frame into desired geometrical layout. From there it creates \textit{N} different video versions, each with a different QEC and encoded in \textit{k} different bitrates, as illustrated in Figure \ref{fig:newdelivery}. The server splits all such encoded video versions into second-long segments that are classified in \textit{N*k} representations (based on their respective resolution, bitrate, and QEC value), enabling the client to regularly switch from one representation to another. The quality of each segment around the QEC is the highest, while the remaining part is encoded in lower quality.

\input{newDelivery.tex}

The client moves the head, while the available bandwidth changes. The possible head movements are forward and backwards, side to side, and shoulder to shoulder, referred to as \emph{pitch}, \emph{yaw}, and \emph{roll}, respectively. Changes in the head orientation modify the FoV center, requiring a new FoV video to be displayed to the user. The client periodically requests a new video segment with the representation that matches the new FoV \textbf{and} the available throughput. Similarly to DASH, the client runs an adaptation algorithm to select the video representation.

\parag{Adaptation algorithm} The client first selects, based on the user's head movements and the information from the MPD file, the QEC of the video. This is an important addition to the DASH bitrate adaptation logic, since the QEC determines the quality of the video that will be delivered and displayed to the user. After selecting the QEX, the client chooses the video representation that contains this QEC and whose bitrate fits the expected throughput for the next X seconds (i.e., X being the segment duration). It sends a request to the server for the new segment in the chosen representation. The server replies with the requested 360-degree video version, from which the client extracts the FoV video, displaying it on the user's HMD. Note that this is possible with the state-of-the-art HMDs \cite{}. %The center of the \ac{FoV} is a point on the sphere, while the size of the FoV depends on the device (typically around 100$^\circ$ in state-of-the-art devices).

\parag{Adaptive video streaming} The adaptive video streaming systems are based on the "bet" that the selected representation will match the configuration for the next X seconds. The requested bitrate needs to match the current throughput for the duration of the segment, otherwise the user might experience the video stalls, interruptions, and delays. In the proposed navigable 360-degree video delivery the bet is not only that the throughput will accommodate the requested bitrate, but also that the chosen QEC will be close to the FOV center for the next X seconds.

\parag{QEC selection} In the current implementation the QEC selection is based on the minimum orthodromic distance\footnote{The shortest distance between two points on the surface of a sphere, measured along the surface of the sphere.} between the FoV center and each available QEC. However, if the user's head moves during this time, the consequences for the user are even worse than with the adaptive streaming, including the degraded video quality. A new FoV video is extrapolated from the existing video version stored on the client. The longer the time between the requests, the longer the potential distance from the old to the new FoV center (i.e., FoV center distance) and the larger the video quality degradation of the displayed FoV video. In order to improve the quality of the displayed video to the user, we plan to predict in the future which of the QECs will be close to FoV center for the segment size duration, given the user's historical head movements.



%The specifics of 360-degree video delivery are that only a portion of the delivered 360-degree video is displayed on the user's HMD (i.e., where the user looks at). Therefore, only this part of the video (around the QEC) should be given in full quality, while the quality of the remaining video can be adjusted to the user's head movements and the viewing probability. A video that is mapped to a given geometric layout is split into viewing faces, each of which corresponds to specific viewing direction and can be assigned a different quality.

\parag{Extending MPD file} To implement the proposed FoV-adaptive video streaming, we extended a DASH manifest file with new information, as illustrated in Listing \ref{mpdChanges}.
\begin{lstlisting}[language=xml, frame=single, backgroundcolor=\color{white}, caption=Extensions of MPD file, label=mpdChanges][hptb]
<?xml version="1.0"?>
   <MPD>
    ...
        <AdaptationSet geometricLayout="cubeMap" qualityArrangement="4/1/1">
            <Representation id="1" qec="90,60" bandwidth="9876" width="1920" height="1080" frameRate="30">
                <SegmentList timescale="1000" duration="2000">
                ...
            </Representation>
        </AdaptationSet>
    </MPD>
</xml>
\end{lstlisting}

Each representation contains the \textbf{QEC coordinates} in degrees, besides the parameters that are already defined in the standard \cite{}. Similarly, an adaptation set is extended with \textbf{geometrical layout} and \textbf{quality arrangement} attributes, for video encoder to know how to encode the video. The geometrical layout is assigned one of the projection types (equirectangular, cubeMap, pyramid or rhombic dodecahadron), while the quality arrangement takes a string value specifying the number of the layout faces encoded in the high/medium/low qualities. The proposed changes are added as new attributes of \textit{Representation} and \textit{AdaptationSet} tags. With such extensions we maintain the conformance with the standard, ensuring the interworking with the legacy DASH clients that can simply ignore these newly introduced attributes.


\parag{Future adaptation algorithm development} Many bitrate adaptation logic algorithms for adaptive streaming have been developed over time to optimize the user experience \cite{}. Most of these algorithms focus on predicting the expected throughput for the next few chunks in order to make optimal bitrate decisions for maximizing the Quality of Experience (QoE). Similarly to different bitrate adaptation algorithms, we expect that 360-degree video adaptation algorithms will be developed in the future to predict the user's head movements for the next few seconds in order to adjust the video segment size and arrange the qualities in 360-degree video accordingly. Exactly how many seconds should pass until receiving a new video version depends on the viewer's head movements, video content (i.e., whether he/she watches a football match, news, or plays a computer game), and the experienced quality degradations variations, which needs to be investigated in the future work.%This head movement prediction will be based on the user's previous head movements and the video content type. If the content is static, the user will not move his/her head so much compared to when watching an action move.


\parag{Video segment size} A video segment size determines how often requests are sent to the server. It typically ranges from 2 seconds to 10 seconds per segment. Short segments are good to quickly adapt to head movement and bandwidth changes, but the video encoding requires a higher number of segments and streaming of this video results in larger manifest files. Shorter segments also increase the network overhead by frequent requests, as well as network delay because of the round trip time that is needed to establish a TCP connection\footnote{In case of non-persistent HTTP connection TCP connection is established with the server after each request.} and request a segment (which can both take significant time in case of very short segment sizes). Longer segments are better in coding efficiency and quality than the shorter ones, however loose on the flexibility to adapt the stream to changes.

A user's head movements affect the "optimal" segment size selection and quality of the displayed video. If the user watches relatively static video content, his/her head will not move that much, leading to slower head movements. Slower head movements usually cause shorter FoV center distances, requiring only a small portion of video to be encoded in high quality (while the rest can be given in the lowest quality) in order to optimize the user experience. They also need less frequent video segment updates. On the contrary, faster head movements typically produce longer FoV center distances,  requiring larger portions of video to be encoded in higher quality and more frequent segment updates to optimize the user experience.
%If the user moves his head a lot, he needs to get more frequently segments with new QECs.

We study, in the remainder of the paper, the impact of the segment size/request rate and the FoV center distance on the viewport quality that is displayed to the user, using a single head motion dataset. More studies with different datasets and video content types are needed in the future to generalize these results. We are also interested to find out how many representations are needed to achieve a desired video quality, using the particular geometrical layout.
