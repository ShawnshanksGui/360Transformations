\section{Introduction}
\label{sec:introduction}

%\subsection{Context and Motivations}
%\label{sec:context}

The popularity of navigable 360-degree video systems
has grown with the advent of omnidirectional capturing systems
%(including multi-camera with stitching systems)
and interactive displaying
systems, like \acp{HMD}.
However, to deliver 360-degree video content on the Internet, the content providers
have to deal with a problem of bandwidth waste: What is displayed in the device,
the so-called \textit{\ac{FoV}}, is only a fraction of what is downloaded, which is an omnidirectional view of the scene.
%New delivery solutions have to be designed to provide high-quality \ac{FoV} video despite 
%As epitomized by the launch of a dedicated activity at the June 2016 MPEG \ac{DASH} meeting,
%new solutions have to be designed to secure high-quality delivery of 360-degree video.
This bandwidth waste is the price to pay for interactivity.
To prevent \emph{simulator sickness}~\cite{moss2011characteristics}
and to provide a good \ac{QoE}, the vendors of \acp{HMD} recommend the multimedia
systems to react
to head movements as fast as the \ac{HMD}
refresh rate.%
\footnote{\url{https://developer.oculus.com/documentation/intro-vr/latest/concepts/bp_intro/}}
Since the refresh rate of
state-of-the-art \acp{HMD} is \SI{120}{Hz},%
\footnote{\url{http://www.vrnerds.de/vr-brillen-vergleich/}}
the whole system should react in less than
\SI{10}{ms}. This delay constraint prevents the implementation of traditional delivery
architecture where the client notifies a server about changes and waits for reception
of adjusted content. Instead, in the current \ac{VR} video delivery systems, the server sends the 
full 360-degree stream, from which the \ac{HMD} 
extracts the \ac{FoV} video in real time with respect to head movements. Most pixels
of the delivered video stream are not used.

Let us provide some numbers to illustrate the problem.
The \ac{FoV} is commonly defined by
a device-specific viewing angle (typically 120 degrees), which delimits horizontally or
vertically the scene from the head direction center, called \ac{FoV} center. To get a good
immersion, the pixel
resolution of
the displayed \ac{FoV} should be high, typically 4k ($3840\times2160$). It means that
the resolution of the full 360-degree video should be at least 12k ($11520\times6480$).
In addition, the immersion requires frame rate in the order of refresh rate, so
typically around 100\,\acp{fps}.
Overall, high-quality 360-degree videos combine both very large resolution (up to 12k)
and very high frame rate (up to 100\,\acp{fps}). To compare, the bit-rate of 8k videos
at 60\,\acp{fps} encoded in \ac{HEVC} is around \SI{100}{Mbps}~\cite{7398367}.

We propose in this paper a solution where, following the same principles as in
rate-adaptive streaming technologies, the server offers multiple \emph{representations}
of the same 360-degree video. But instead of offering representations that only differ by
their bit-rate, the server offers here representations that differ by
having a better quality in a given region of the video.
Our proposal is a \emph{FoV-adaptive streaming system} and is depicted in Figure~\ref{fig:deliverychain}.
Each representation is characterized by a \emph{\ac{QEC}}, which is a given
position in the spherical video. Around the \ac{QEC}, the quality of the video is maximum,
while it is lower for video parts that are far from the \ac{QEC}. Similarly as
in \ac{DASH}, the video is cut into segments and the client periodically runs
an \emph{adaptive algorithm} to select
a video representation for the next segment. In a \ac{FoV}-adaptive system, the clients select
the video representation such that, not only the bit-rate fits with their bandwidth, but also
the \ac{QEC} is close to their \ac{FoV} center.

\input{deliveryChain.tex}

This \ac{FoV}-adaptive 360-degree streaming system has three advantages:
$(i)$ the bit-rate of the delivered video is lower than the original full-quality video because the
parts of the video that are far from the \ac{QEC} are encoded at low quality.
$(ii)$ When the end-user does not move, the \ac{FoV} video is extracted from the highest
quality spherical video.
And $(iii)$ when the end-user moves the head, the device can
always extract
a new \ac{FoV} video because it has the full spherical video. If the
new \ac{FoV} center is far from the
\ac{QEC} of the video representation, the quality of the extracted video is lower but this
degradation is transient until the
device selects another video representation with a closer \ac{QEC}.

In this paper, we introduce \ac{FoV}-adaptive streaming systems for navigable 360-degree
videos. 
We discuss some key design points. First, we present the overall processes that have to
be implemented at both client and server side, and we show how our proposal can be integrated
into the MPEG \ac{DASH} standard. Our proposal is thus a contribution to the 
\ac{VR} group that was launched by MPEG \ac{DASH} in May 2016.
Second, we address the choice of the geometric layout into which the spherical video should
be projected for encoding. We evaluate 
several video quality arrangement into the geometric layouts and we show that the
cube map layout offers the best performance regarding the quality of the extracted
\ac{FoV}.
Third, we study  the length of the video segments
for \ac{FoV}-adaptive streaming system. Based on a dataset of
real users navigating in 360-degree videos, we show that head movements are significant
in short period, so the video segments have to be short enough to enable
frequent \ac{QEC} switches.
Finally, we introduce a tool (released on open source in a public
repository\footnote{\url{https://github.com/xmar/360Transformations}}),
which create video representations for
\ac{FoV}-adaptive 360-degree videos.
The tool is highly configurable: from a given 360-degree video, it allows
any arrangement of video quality depending on any geometric layout, and it
extracts the
\ac{FoV} video from any point in the sphere. This tool thus provides the
main software modules for the implementation of \ac{FoV}-adaptive streaming
for navigable 360-degree videos.

%is characterized by an \emph{angle of vision}. It contains the full spherical
%video with an emphasis on the given angle of vision, \textit{i.e.},
%the part of the video that is in front of the angle of vision is at the highest quality and the video quality
%degrades for the other parts.

%\subsection{Our Contributions}
%
%%
%%of spherical videos into various geometrical layouts. We propose
%%several ways to arrange the video qualities, which leverage the structure of the
%%different layouts.
%We study the navigable 360-degree video delivery system with a
%focus on the arrangement of video qualities within the projections of
%spherical videos into geometric layouts. Our contribution is twofold:
%\begin{itemize}
%\item
%\item We provide a comprehensive comparison of the different geometric arrangements of
%video quality on 360-degree video. We address the main questions raised
%by~\citet{facebook}: which layout provides the best trade-off
%between bit-rate and
%quality, how many video representations should be offered by the server, and what geometric
%quality arrangement
%enables the smoothest navigation.
%\end{itemize}
%
%In this short paper, we restrict our study to essentials, and we provide only our main findings based on a
%sample of the results. \textit{Here are our findings...}
%
%


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
