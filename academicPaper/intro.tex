\section{Introduction}
\label{sec:introduction}

\subsection{Context and Motivations}

The popularity of interactive 360-degree video systems (also known as immersive omnidirectional video) 
has grown with the advent of both capturing systems
(including catadioptric optical systems and multi-camera with stitching systems) and video
consumption systems (including head-mounted display and interactive HTML5 video players).
However, the delivery of 360-degree video content, from the servers of the content providers
to the end-users,
is still a challenge. Head-mounted devices display two videos (one per
eye), each of them with a high resolution (typically $1080\times 1200$ for state-of-the-art devices)
and a high frame rate. Both videos, which correspond to the \ac{FoV} on both eyes, are extracted
from a wider spherical video with a resolution that is three to four times larger.

None of the current solutions for the delivery of 360-degree videos is entirely satisfactory. Sending only 
the \ac{FoV} videos is the least bandwidth-hungry implementation. It requires however the server to 
compute the \ac{FoV} from the spherical video for each end-user. Moreover, it does not enable fast
navigation within the scene: When the client changes the 
\ac{FoV}, the device cannot immediately display any video because it does not have information on the other
parts of the full spherical video. The device has to notify the server and to wait for the reception of the 
newly adjusted \ac{FoV} videos. Another delivery implementation is to send the full spherical video 
and to let the device
extract the \ac{FoV} videos. This solution enables fast navigation but the bandwidth requirements are 
significant.

We explore in this paper a solution where the server offers multiple \emph{versions} of the same 
360-degree  video. Each version is characterized by an \emph{angle of vision}. It contains the full spherical 
video with an emphasis on the given angle of vision, \textit{i.e.},
the part of the video that is in front of the angle of vision is at the highest quality and the video quality 
degrades for the other parts. The client device chooses the video version according to the eye attention 
such that the 
the front view of the video version is close to the \ac{FoV} videos. Thus, the \ac{FoV} extraction is done
from the high quality spherical video. When the end-user changes the 
\ac{FoV}, the device can always extract a \ac{FoV} video (sometimes from a low quality part of the 
full video) until it switches to a new video version that better suits the new visual attention.

\subsection{Limitations of Previous Work}

The principle of differentiated video quality in the delivery of omnidirectional videos has been sketched 
in a recent short paper by~\citet{ochi_live_2015}. Their proposal is based on the idea of video
tiling, which has been implemented for navigable panorama 
video~\cite{sanchez_compressed_2015,wang_mixing_2014,gaddam_tiling_2015}: The panorama video 
is cut into independent \emph{tiles} (typically $8\times 8$) and the server offers several
video qualities for each tile. The client selects the quality of each tile according to the eye attention. This
solution has the same advantages of our proposal, since it reduces the overall bandwidth by emphasizing
the video quality only for the parts that are actually watched, at the price of transient lower quality just after
brutal navigation events. However, this solution does not take into account the characteristics of 360-degree
videos. A spherical video can be mapped into an \emph{equirectangular} video without losing any information
for the extraction of \ac{FoV}, but the pixels at the poles are over-sampled (the number of pixels
used in the rectangle to depict a pixel into the sphere is higher at the pole than in the equator). 
This characteristic is known to degrade the
performance of traditional video encoders~\cite{wojciechowski_h.264_2006,yu_framework_2015}. It also 
makes equirectangular tiling less relevant.

The mapping of spherical videos into 2D video is a projection that generates distortion on the resulting
full 2D video, but some projections enable \ac{FoV} extraction without information loss. It is typically the case of 
projections on \emph{cube map}~\cite{Ng2005} and 
\emph{rhombic dodecahedron}~\cite{fu_rhombic_2009}. The previous work regarding mapping into these
geometrical structures has focused on enabling efficient implementation of signal processing 
functions~\cite{kazhdan_metric-aware_2010} and improving the video encoding~\cite{tosic_low_2009}. 
However, to the best of our knowledge, the 
question of differentiated quality on the different faces of the structures has not been studied so far.

Finally, a major content provider of 360-degree videos has recently released details about the 
implementation of its delivery platform~\cite{facebook}. The depicted system is based on the same
main idea as our proposal, where up to 30 versions of the same video are available according to angles
of vision. This description corroborates that, from an industrial perspective, the extra-cost of
generating and storing multiple versions of the same video is relevant with respect to the bandwidth
savings. The authors use a mapping of the spherical videos into a pyramid where the base is the front
view and the peak is behind. Yet, this mapping under-samples some pixels, which results in 
information loss and distorted extraction of \ac{FoV} videos.

\subsection{Our Contributions}


%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
