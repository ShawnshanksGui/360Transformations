\section{Background and Related Work}
\label{sec:related}

We introduce the necessary geometric concepts for spherical
videos, and discuss prospective architecture
proposals for navigable $360$-degree video delivery.

\subsection{Geometric Layouts for 360-degree Videos}

A $360$-degree video is captured in every direction from a unique point,
so it is essentially a \emph{spherical} video. Since current video encoders
operate on a two-dimensional rectangular image, a key step of the
encoding chain is to project the spherical video onto a planar
surface. The projection of a sphere onto a plane (known as mapping)
has been studied for centuries. In this paper, we consider the four
projections that are the most discussed for $360$-degree video
encoding~\cite{yu_framework_2015}. These layouts are depicted in
Figure~\ref{fig:mapping}.
From the images that are projected on an \textit{equirectangular}
panorama, a \textit{cube map}, and a \textit{rhombic dodecahedron}, it
is possible to generate a viewport for any position and angle in the
sphere without any information loss~\cite{Ng2005, fu_rhombic_2009}.
However, some pixels are over-sampled (a pixel on the sphere is
projected to a pair of pixels in the projected image). This is
typically the case for the sphere pole when projected on the
equirectangular panorama. This over-sampling degrades the performance
of traditional video encoders~\cite{wojciechowski_h.264_2006,
yu_framework_2015}. On the contrary, the projection into a pyramid
layout causes under-sampling: some pairs of pixels on the sphere
are merged into a
single pixel in the projected image by interpolating their color
values. This under-sampling cause distortion and information loss in
some extracted \FoV{}s. Previous work regarding projection of spherical
videos into different geometric layouts focuses on enabling efficient
implementation of signal processing
functions~\cite{kazhdan_metric-aware_2010} and improving the video
encoding~\cite{tosic_low_2009}.

\tikzsetnextfilename{2d_layout_projections}
\begin{figure}[t]
\centering
\begin{tikzpicture}
\def\sizeSphere{20}\def\ecartY{-1.2}\def\ecartX{6}

%% da sphere
\pic [local bounding box=spher]  at (0,0) {spherical=15};

%% recantagular
\pic [local bounding box=equi] at (-3,\ecartY) {equirectangular={\sizeSphere}{-1}{0}};

%% cupe map
\pic [local bounding box=cubemap] at (-1,\ecartY) {cubemap=\sizeSphere};

%% pyramid
\pic [local bounding box=pyra] at (1,\ecartY) {pyramid=\sizeSphere};

%% rhombic
%%\pgfdeclareimage[width=36 pt]{dodecahedron}{RhombicDodecahedron.png}
%%\node at (3,\ecartY) (dodec)
%%    {\pgfbox[center,center]{\pgfuseimage{dodecahedron}}};

\def\unitused{0.22}

\pic [local bounding box=dodeca] at (3,0.88*\ecartY) {dodecahedron=\unitused};

%% links
\draw[-latex] (spher.180) -| (equi);
\draw[-latex] (spher.200) -| (cubemap);
\draw[-latex] (spher.340) -| (pyra);
\draw[-latex] (spher) -| (dodeca);

\node[font=\scriptsize,anchor=north] at (equi.south) {equirectangular};
\node[font=\scriptsize,anchor=north] at (cubemap.south) {cube map};
\node[font=\scriptsize,anchor=north] at (pyra.south) {pyramid};
\node[font=\scriptsize,anchor=north] at (dodeca.south) {\vphantom{y}dodecahedron};

\end{tikzpicture}
\caption{Projections into four geometric layouts}\label{fig:mapping}
\end{figure}

\parag{Our contributions}We propose to leverage the geometric
structure of the layouts to implement a video encoding based on
\ac{QEC}. Each geometric layout is characterized by a number of
\emph{faces} (\textit{e.g.}, 6 for the cube map, 12 for the
dodecahedron) and a given \emph{central point} (which corresponds to a
position on the sphere).
From a given central point and a given layout, our idea is to encode
the front face in full quality while the quality of other faces is
reduced.
To our knowledge, such idea has not been studied yet.
Another originality of our work is that we measure \ac{QoE} by
measuring the quality of several extracted viewports instead of
the full projected video.

\subsection{Personalized Viewport-Only Streaming}

An intuitive idea to address the problem of resource waste due to the
delivery of non-displayed video data is to stream only the part of the video that
corresponds to the viewport. This solution however does not enable
fast navigation within the $360$-degree video: When the client moves the
head, the \FoV{} center changes, requiring a new viewport to be
immediately displayed. Since the device has no knowledge about other
parts of the spherical video, it has to notify the server about the
head movement and wait for the reception of the newly adjusted viewport.
As seen in other interactive multimedia
systems~\cite{ChoyWSR14}, this solution cannot meet the \SI{10}{ms} latency
requirement in the standard Internet, even with the assistance of
\ac{CDN}. In addition, this solution requires the server to extract a
part of the video (thus to spend computing resources) for each client
connection.

\parag{Our contributions}In our system, the server always delivers
the full video, but it has different versions of this video depending
on the \ac{QEC} (characterized by the \ac{QEC}). The client device
selects the right representation and extracts the viewport. The
storage requirements at the server side increase but all the
processing is done at the client side (representation selection and
viewport extraction). This idea matches the adaptive delivery
solutions that content providers have recently adopted (\textit{e.g.}~\ac{DASH}), trading client-personalized delivery for
simple server-side management operation.

\subsection{Tiling for Adaptive Video Streaming}
To deal with the cases of end-users consuming only a fraction of the
video (navigable
panorama~\cite{sanchez_compressed_2015,wang_mixing_2014,gaddam_tiling_2015}
and large-resolution video~\cite{jean16mmsys}), the most studied delivery solution
leverages the concept
of \emph{tiling}.
The idea is to
spatially cut a video into independent tiles. The server offers
multiple video representations of each tile; the client periodically
selects a representation for each tile and it has to reconstruct the
full video from these tiles before the viewport extraction.
In a short paper,~\citet{ochi_live_2015} have sketched a
tile-based streaming system for $360$-degree videos. In their proposal,
the spherical video is mapped onto an \emph{equirectangular} video,
which is cut into $8\!\times\! 8$ \emph{tiles}.

A tile-based adaptive streaming system provides the same features as
our proposed system regarding navigability (the clients get the full
video), bandwidth waste reduction (the video at low quality for
non-\FoV{} part) and \ac{QoE} maintenance (the downloaded video is
at full quality near the \FoV{} center). It has however several
critical weaknesses. First, the client has to first reconstruct the
video from independent tiles before the viewport extraction can take
place, which requires energy and time spent for each video frame.
Second, the more tiles there are, the less efficient the video
encoding is due to the tile
independence~\cite{sanchez_compressed_2015}. Third, the management at
the server is heavier because the number of files is larger. For
example, a typical $8\times8$ tiling offered at six quality levels
contributes to having $384$~independent files for each video segment,
and this results in larger \ac{MPD} files (or manifest
files). Finally, the management at the client side is heavier, as
well. For each tile, the client should run a representation selection
process and manage a specific network connection with the server.

\parag{Our contributions}In our system, the server
prepares $n$ \ac{QEC}-based videos, each of them being a
pre-processed set of tile representations. Each \ac{QEC}-based video is then
encoded at $k$ \emph{global} quality levels.
The main advantages include
an easier management for the server (fewer files hence a smaller
\ac{MPD} file), a simpler selection process for the client (by a
distance computation), and no need for re-constructing the video before
the viewport extraction.

\subsection{QEC-Based Streaming}

A  provider of $360$-degree videos (Facebook) has recently
released details about the implementation of its delivery
platform~\cite{facebook}.
The spherical video is projected onto a pyramid layout from multiple
(up to $30$) central points to generate a set of video representations.
Since the
front face of pyramid projection has a better image quality than the other faces, the
system is in essence similar as our concept of \ac{QEC}. The end-users
periodically select one of the representations
based on their \FoV{} center. This implementation
corroborates that, from an industrial perspective, the extra-cost of
generating and storing multiple \ac{QEC}-based representations of the
same video is compensated by bandwidth savings and
enhanced system usability.
However, as we will see in Section~\ref{sec:settings}, the pyramid projection is not
the best regarding the viewport quality. Moreover, the system
uses the same video quality on each face, which is less
efficient than our proposal. Finally, the impact of the video encoding on the solution
is not given.

\parag{Our contributions}Our approach is based on the same
idea of offering multiple \ac{QEC}-based video representations.
However, we provide a complete study of our system with the additional
distinction of having varying quality across the geometrical layout. Moreover,
our study includes an evaluation of several
geometric layouts, an analysis of the best segment duration, an
analysis of the best number of \acp{QEC},
and a step towards integration into MPEG \ac{DASH}.
